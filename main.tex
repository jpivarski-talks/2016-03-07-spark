\documentclass{beamer}
%
% Choose how your presentation looks.
%
% For more themes, color themes and font themes, see:
% http://deic.uab.es/~iblanes/beamer_gallery/index_by_theme.html
%
\mode<presentation>
{
  \usetheme{default}      % or try Darmstadt, Madrid, Warsaw, ...
  \usecolortheme{default} % or try albatross, beaver, crane, ...
  \usefonttheme{default}  % or try serif, structurebold, ...
  \setbeamertemplate{navigation symbols}{}
  \setbeamertemplate{caption}[numbered]
  \setbeamertemplate{footline}[frame number]
} 

\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}

\title[2016-03-07-spark]{ROOT-Spark integration}
\author{Jim Pivarski}
\institute{Princeton Univeristy --- DIANA}
\date{March 7, 2016}

\xdefinecolor{darkblue}{rgb}{0.1,0.1,0.7}

\begin{document}

\begin{frame}
  \titlepage
\end{frame}

% Uncomment these lines for an automatically generated outline.
%\begin{frame}{Outline}
%  \tableofcontents
%\end{frame}

\begin{frame}{Strategy}
\begin{block}{Scope}
Spark is just one big data technology, but almost all of them share a common difficulty: the data backbone is implemented in the Java Virtual Machine (JVM) and ROOT is C++.

\vspace{0.2 cm}
This doesn't mean that users must use Java (I use a mix of Scala and Python), but it does mean that data must be efficiently converted into a JVM-friendly format to be used at a large scale.
\end{block}

\begin{block}{Methods}
I've been keeping three options open:
\begin{enumerate}
\item Pure Java using FreeHEP (reimplementation of ROOT in Java by Tony Johnson).
\item Direct ROOT C++ $\to$ JVM connection with JNI (or an abstraction layer on top of it, such as JNA).
\item ROOT C++ as an external process, passing serialized data through a pipe.
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Status}
\begin{enumerate}
\item Pure Java using FreeHEP: works, but I have concerns about its efficiency for non-local files (e.g. xrootd). When all three options are ready, this method should be featured in a performance study.

\item Direct ROOT C++ $\to$ JVM: this now works as well. The random segmentation faults I was experiencing were related to interference between ROOT's signal handlers and Java's signal handlers.

\item ROOT C++ as an external process: of course this should work, and I've been putting my efforts here so that we at least have a working baseline. I'm using Avro to stream data from the ROOT process to the JVM process. Avro is an efficient binary format with bindings in many languages.
\end{enumerate}

I'm further considering a merger of options \textcolor{darkblue}{\#2} and \textcolor{darkblue}{\#3}: in addition to the Avro bridge, we could run ROOT as an external process with data transferred via shared memory (``off-heap,'' a technique used by high-frequency traders).
\end{frame}

\begin{frame}{Status, continued}
What have I actually been doing?

\vspace{0.2 cm}
Consuming the entire ROOT data model and providing the appropriate translations to Avro. Required for both \textcolor{darkblue}{\#2} and \textcolor{darkblue}{\#3}.

\begin{itemize}
\item Simple primitive branches (TLeaf).
\item Primitive arrays, multidimensional arrays preserving substructure: e.g.\ ``{\tt Double32\_t fMatrix[4][4]}'' becomes a four-element array containing four-element arrays of numbers.
\item Primitive arrays with an external counter variable, such as {\tt fNTracks}.
\item STL vectors (and maps? does ROOT use {\tt std::map}?).
\item User-defined classes, such as the {\tt Event} example, CMSSW data with FWLite, ART data with the appropriate libraries.
\item Pointers as nullable objects: e.g.\ ``{\tt Double32\_t *fClosestDistance}'' becomes ``{\tt ["null", "double"]},'' a value that could be null ({\tt None} in Python) or a number.
\end{itemize}

\vspace{0.2 cm}
(See \url{https://github.com/diana-hep/root2avro}.)
\end{frame}

\begin{frame}[fragile]{How do I expect users to use this?}
This is infrastructure necessary to make an efficient Spark InputRDD. The user can use Spark directly, PySpark, or SparkR as he or she sees fit. However,

\begin{itemize}
\item For big jobs, like ntuple-skimming (my target use-case), they'll find Spark's JVM-to-Python bridge (Py4J) to be a source of inefficiency.

\vspace{0.2 cm}
It would be much more efficient to do
\begin{verbatim}
dataset.filter(event => event.numMuons >= 2  &&
                        event.muons(2).pT > 20)
\end{verbatim}
on the Scala side before attempting heavy interactive analysis in Python. (Note the difference in syntax above.)

\item If analysts really need one language for everything, it wouldn't be hard to provide a Jython skin over native Spark, to be used only for big datasets (skimming).
\end{itemize}
\end{frame}

\end{document}
